{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "import keras.backend as K\n",
    "from keras.engine.topology import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soft-attention\n",
    "\n",
    "def unchanged_shape(input_shape):\n",
    "    # function for Lambda layer \n",
    "    return input_shape\n",
    "\n",
    "def soft_attention_alignment(input_1, input_2):\n",
    "    \"\"\"\n",
    "    两输入为三维张量(bs, ml1, size) (bs, ml2, size)\n",
    "    \n",
    "    return (bs, ml2, size), (bs, ml1, size)\n",
    "    \"\"\"\n",
    "    attention = Dot(axes=-1)([input_1, input_2])  # (bs, ml1, size)·(bs, ml2, size) ==> (bs, ml1, ml2)\n",
    "    \n",
    "    w_att_1 = Lambda(lambda x: K.softmax(x, axis=1), output_shape=unchanged_shape)(attention)  # (bs, ml1, ml2)\n",
    "    w_att_2 = Permute((2, 1))(Lambda(lambda x: K.softmax(x, axis=2), \n",
    "                                     output_shape=unchanged_shape)(attention))  # (bs, ml2, ml1)\n",
    "    \n",
    "    in1_aligned = Dot(axes=1)([w_att_1, input_1])  # (bs, ml1, ml2)·(bs, ml1, size)  ==> (bs, ml2, size)\n",
    "    in2_aligned = Dot(axes=1)([w_att_2, input_2])  # (bs, ml2, ml1)·(bs, ml2, size)  ==> (bs, ml1, size)\n",
    "\n",
    "    return in1_aligned, in2_aligned   # (bs, ml2, size)  (bs, ml1, size)  与输入shape相反\n",
    "\n",
    "# # 测试\n",
    "# a = K.ones((3, 5, 7))\n",
    "# b = K.ones((3, 20, 7))\n",
    "# res1, res2 = soft_attention_alignment(a, b)\n",
    "# print(K.int_shape(res1), K.int_shape(res2))\n",
    "# # >>>(3, 20, 7) (3, 5, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# co-attention\n",
    "\n",
    "def co_attention(input_1, input_2):\n",
    "    \"\"\"\n",
    "    两输入为三维张量(bs, ml, size) (bs, ml, size)  (要求步长相同)\n",
    "    \n",
    "    return \n",
    "    \"\"\"\n",
    "    dense_w = TimeDistributed(Dense(1))\n",
    "    atten = Lambda(lambda x: K.batch_dot(x[0], x[1]))([input_1, Permute((2, 1))(input_2)]) \n",
    "    # (bs, ml, size), (bs, size, ml)  ==>  (bs, ml, ml)\n",
    "\n",
    "    atten_1 = dense_w(atten)   # (bs, ml, 1)\n",
    "    atten_1 = Flatten()(atten_1)   # (bs, ml)\n",
    "    atten_1 = Activation('softmax')(atten_1)\n",
    "    atten_1 = Reshape((1, -1))(atten_1)   # (bs, 1, ml)\n",
    "    \n",
    "    atten_2 = dense_w(Permute((2, 1))(atten))   # (bs, ml, 1)\n",
    "    atten_2 = Flatten()(atten_2)\n",
    "    atten_2 = Activation('softmax')(atten_2)\n",
    "    atten_2 = Reshape((1, -1))(atten_2)   # (bs, 1, ml)\n",
    "    \n",
    "    out1 = Lambda(lambda x: K.batch_dot(x[0], x[1]))([atten_1, input_1])  # (bs, 1, ml)·(bs, ml, size)  ==> (bs, 1, size)\n",
    "    out1 = Flatten()(out1)   # (bs, size)\n",
    "    out2 = Lambda(lambda x: K.batch_dot(x[0], x[1]))([atten_2, input_2])  # (bs, 1, ml)·(bs, ml, size)  ==> (bs, 1, size)\n",
    "    out2 = Flatten()(out2)   # (bs, size)\n",
    "    \n",
    "    return out1, out2  # (bs, size), (bs, size)\n",
    "\n",
    "# # 测试\n",
    "# a = K.ones((3, 5, 7))\n",
    "# b = K.ones((3, 5, 7))\n",
    "# res1, res2 = co_attention(a, b)\n",
    "# print(K.int_shape(res1), K.int_shape(res2))\n",
    "# # >>>(3, 7) (3, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 层级attention\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':   # 默认添加最后一个维度  return => (samples, steps, feaures)\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)   # (samples, steps, features), (feaures, feaures, 1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.  # 支持masking\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention  # 用上下文向量支持attention\n",
    "    \n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.  \n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.  \n",
    "    \n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.  # dimension based on GRU shape\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())  # [None， features]\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3  # 输入长度必须为3 \n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),  # 相同 features  (size, size)\n",
    "                                 initializer=self.init,  # initializer.get('glorot_uniform')\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,  # 正则化\n",
    "                                 constraint=self.W_constraint)   # 约束\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),  # (size,)\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),   # (size, )\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)  # .build(input_shape) \n",
    "\n",
    "    def call(self, x):\n",
    "        uit = dot_product(x, self.W)  # (bs,ml,size)(size,size)  ==> (bs, ml, size)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b   # (bs, ml, size)\n",
    "\n",
    "        uit = K.tanh(uit)  # (bs, ml, size)  得到uit\n",
    "        ait = dot_product(uit, self.u)   # (bs, ml, size), (size, 1)  ==> (bs, ml, 1)  => # (bs, ml)\n",
    "\n",
    "        a = K.softmax(ait)  # (bs, ml)\n",
    "        a = K.expand_dims(a)     # (bs, ml)\n",
    "        weighted_input = x * a   #  (bs, ml, size) * ((bs, ml, 1) => (bs, ml, size)\n",
    "\n",
    "        return K.sum(weighted_input, axis=1)   # (bs, size)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]   # 不用括号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-head self-attention  来源：https://spaces.ac.cn/archives/4765\n",
    "\n",
    "class Position_Embedding(Layer):  # 位置embedding\n",
    "    \n",
    "    def __init__(self, size=None, mode='sum', **kwargs):\n",
    "        self.size = size #必须为偶数   可以自定义位置embedding的维度\n",
    "        self.mode = mode\n",
    "        super(Position_Embedding, self).__init__(**kwargs)\n",
    "        \n",
    "    def call(self, x):   # (bs, ml, size)\n",
    "        if (self.size == None) or (self.mode == 'sum'):\n",
    "            self.size = int(x.shape[-1])   # size\n",
    "        batch_size,seq_len = K.shape(x)[0],K.shape(x)[1]   # bs, ml\n",
    "        position_j = 1. / K.pow(10000., 2*K.arange(self.size/2, dtype='float32') / self.size)   # (size/2,)\n",
    "        position_j = K.expand_dims(position_j, 0)  # (1, size/2)\n",
    "        position_i = K.cumsum(K.ones_like(x[:,:,0]), 1) - 1   #K.arange不支持变长，只好用这种方法生成  (bs, ml)\n",
    "        position_i = K.expand_dims(position_i, 2)   # (bs, ml, 1)\n",
    "        position_ij = K.dot(position_i, position_j)  # (bs, ml, 1) · (1, size/2)  ==>  (bs, ml, size/2)\n",
    "        position_ij = K.concatenate([K.cos(position_ij), K.sin(position_ij)], 2)   # (bs, ml, size)\n",
    "        if self.mode == 'sum':\n",
    "            return position_ij + x   # (bs, ml, size)\n",
    "        elif self.mode == 'concat':\n",
    "            return K.concatenate([position_ij, x], 2)   # (bs, ml, size*2)\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.mode == 'sum':\n",
    "            return input_shape\n",
    "        elif self.mode == 'concat':\n",
    "            return (input_shape[0], input_shape[1], input_shape[2]+self.size)\n",
    "\n",
    "\n",
    "# 多头自注意力\n",
    "class Attention(Layer):\n",
    "\n",
    "    def __init__(self, nb_head, size_per_head, mask_right=False, **kwargs):\n",
    "        self.nb_head = nb_head   # 注意力头数\n",
    "        self.size_per_head = size_per_head   # 每个注意力头的大小\n",
    "        self.output_dim = nb_head*size_per_head   # 输出维度\n",
    "        self.mask_right = mask_right   # 是否mask\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.WQ = self.add_weight(name='WQ', \n",
    "                                  shape=(input_shape[0][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)   # (size, att_dim)\n",
    "        self.WK = self.add_weight(name='WK', \n",
    "                                  shape=(input_shape[1][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)   # (size, att_dim)\n",
    "        self.WV = self.add_weight(name='WV', \n",
    "                                  shape=(input_shape[2][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)   # (size, att_dim)\n",
    "        super(Attention, self).build(input_shape)\n",
    "        \n",
    "    def Mask(self, inputs, seq_len, mode='mul'):\n",
    "        if seq_len == None:\n",
    "            return inputs\n",
    "        else:\n",
    "            mask = K.one_hot(seq_len[:,0], K.shape(inputs)[1])   # (bs, ml)\n",
    "            mask = 1 - K.cumsum(mask, 1)  # (bs, ml)\n",
    "            for _ in range(len(inputs.shape)-2):\n",
    "                mask = K.expand_dims(mask, 2)  # (bs, ml, 1)\n",
    "            if mode == 'mul':\n",
    "                return inputs * mask  \n",
    "            if mode == 'add': \n",
    "                return inputs - (1 - mask) * 1e12\n",
    "                \n",
    "    def call(self, x):\n",
    "        #如果只传入Q_seq,K_seq,V_seq，那么就不做Mask\n",
    "        #如果同时传入Q_seq,K_seq,V_seq,Q_len,V_len，那么对多余部分做Mask\n",
    "        if len(x) == 3:\n",
    "            Q_seq,K_seq,V_seq = x\n",
    "            Q_len,V_len = None,None\n",
    "        elif len(x) == 5:\n",
    "            Q_seq,K_seq,V_seq,Q_len,V_len = x\n",
    "        #对Q、K、V做线性变换\n",
    "        Q_seq = K.dot(Q_seq, self.WQ)  # (bs, ml, size) (size, att_dim)  ==>  (bs, ml, att_dim)\n",
    "        Q_seq = K.reshape(Q_seq, (-1, K.shape(Q_seq)[1], self.nb_head, self.size_per_head))  # (bs, ml, nb_head, size_ph)\n",
    "        Q_seq = K.permute_dimensions(Q_seq, (0,2,1,3))   # (bs, nb_head, ml, size_ph)\n",
    "        K_seq = K.dot(K_seq, self.WK)  \n",
    "        K_seq = K.reshape(K_seq, (-1, K.shape(K_seq)[1], self.nb_head, self.size_per_head))\n",
    "        K_seq = K.permute_dimensions(K_seq, (0,2,1,3))\n",
    "        V_seq = K.dot(V_seq, self.WV)\n",
    "        V_seq = K.reshape(V_seq, (-1, K.shape(V_seq)[1], self.nb_head, self.size_per_head))\n",
    "        V_seq = K.permute_dimensions(V_seq, (0,2,1,3))\n",
    "        #计算内积，然后mask，然后softmax\n",
    "        A = K.batch_dot(Q_seq, K_seq, axes=[3,3]) / self.size_per_head**0.5   # (bs, nb_head, ml, ml)\n",
    "        A = K.permute_dimensions(A, (0,3,2,1))   # (bs, ml, ml, nb_head)\n",
    "        A = self.Mask(A, V_len, 'add')\n",
    "        A = K.permute_dimensions(A, (0,3,2,1))   # (bs, nb_head, ml, ml)\n",
    "        if self.mask_right:\n",
    "            ones = K.ones_like(A[:1, :1])\n",
    "            mask = (ones - K.tf.matrix_band_part(ones, -1, 0)) * 1e12\n",
    "            A = A - mask\n",
    "        A = K.softmax(A)   # (bs, nb_head, ml, ml)\n",
    "        #输出并mask\n",
    "        O_seq = K.batch_dot(A, V_seq, axes=[3,2])  # (bs,nb_head,ml,ml) (bs,nb_head,ml,size_ph) => (bs,nb_head,ml,size_ph)\n",
    "        O_seq = K.permute_dimensions(O_seq, (0,2,1,3))   # (bs,ml,nb_head,size_ph)\n",
    "        O_seq = K.reshape(O_seq, (-1, K.shape(O_seq)[1], self.output_dim))  # (bs,ml,att_dim)\n",
    "        O_seq = self.Mask(O_seq, Q_len, 'mul')\n",
    "        return O_seq\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1], self.output_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
